#1 installation overview : https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/

#1.1 crun : source based compilation and installation :
               git@github.com:wt2017/crun-wyou.git based on https://github.com/containers/crun
               
#1.2 cri-o : source based compilation and installation :
               git@github.com:wt2017/crio-wyou.git
             based on :
               https://github.com/cri-o/cri-o/blob/main/install.md#build-and-install-cri-o-from-source
               https://github.com/cri-o/cri-o/blob/main/install.md#starting-cri-o
                     If you are installing for the first time, generate and install configuration files with: sudo make install.config
                     You can run it manually there, or you can set up a systemd unit file with: sudo make install.systemd
               on some environment, conmon (https://github.com/containers/conmon) is mandatory
     Note:
         podman is the tool for cri-o !!! don't use docker !
         replace every docker command with podman !
         It's also important to execute podman as root since rootless images are stored somewhere else !

#1.3 go : install go in /usr/local/go : https://go.dev/doc/install

#1.4 cni : compile and install cni : document in contrib/cni/* under git@github.com:wt2017/crio-wyou.git
                                     source repository : https://github.com/containernetworking/plugins

#1.5 swap : turn off permenently : https://unix.stackexchange.com/questions/674196/cannot-disable-swap

[optional] #1.5 kubectl : install guide : https://kubernetes.io/docs/tasks/tools/
Rationale: step #1.6 includes kubectl together

#1.6 kubeadm kubelet kubectl : install guide : https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
                                     api guide : https://kubernetes.io/docs/reference/setup-tools/kubeadm/
     [before fedora 41] sudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes
     [since fedora 41 Versioned Kubernetes RPMS is introduced] sudo dnf install kubernetes1.31 kubernetes1.31-kubeadm kubernetes1.31-client
     sudo systemctl enable --now kubelet

[nothing to do] #1.8 cgroup driver : update cgroupDriver value from cgroupfs to systemd
Rationale: In v1.22 and later, if the user does not set the cgroupDriver field under KubeletConfiguration, kubeadm defaults it to systemd




#2. cluster creation overview : https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/
#2.1 kernel module installation:
            cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
            > overlay
            > bridge
            > br_netfilter
            > EOF

            cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
            > net.bridge.bridge-nf-call-iptables  = 1
            > net.bridge.bridge-nf-call-ip6tables = 1
            > net.ipv4.ip_forward                 = 1
            > EOF


#2.2 sudo kubeadm init --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=172.16.10.20 // ip on lo or ens interface
     sudo kubeadm init --cri-socket unix:///var/run/crio/crio.sock --kubernetes-version=v1.31.0-dirty --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=172.16.10.20
     #2.2.1
     --cri-socket unix:///var/run/crio/crio.sock to set the crio socket when multiple ones exsit, e.g. containerd.sock by docker

     #2.2.2
     PauseImage repository can't be changed by parameter --image-repository, so one work around is made as.
     Note: have a try w/o below workaround since it could be fixed by later released code.

           wyou@fedora:~/Kubernetes/crio/cri-o$ git diff
           diff --git a/internal/storage/runtime.go b/internal/storage/runtime.go
           index 27f9dac74..c47b7c1c4 100644
           --- a/internal/storage/runtime.go
           +++ b/internal/storage/runtime.go
           @@ -320,6 +320,7 @@ func (r *runtimeService) CreatePodSandbox(systemContext *types.SystemContext, po
                                   DestinationCtx: systemContext,
                           })
                           if err != nil {
           +                       logrus.Errorf("WYOU: Failed to PullImage %q", pauseImage)
                                   return ContainerInfo{}, err
                           }
                           _, img, err = r.storageTransport.ResolveReference(ref)
           diff --git a/pkg/config/config.go b/pkg/config/config.go
           index a5b740611..72119b8c7 100644
           --- a/pkg/config/config.go
           +++ b/pkg/config/config.go
           @@ -1424,6 +1424,8 @@ func (c *ImageConfig) Validate(onExecution bool) error {
 
            // ParsePauseImage parses the .PauseImage value as into a validated, well-typed value.
            func (c *ImageConfig) ParsePauseImage() (references.RegistryImageReference, error) {
           +       // WYOU : hardcode the PauseImage repository as workaround
           +       c.PauseImage = "registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.9"
                   return references.ParseRegistryImageReferenceFromOutOfProcessData(c.PauseImage)
            }

     #2.2.3
     --pod-network-cidr=10.244.0.0/16 is mandatory for flannel network cni !!!

     #2.2.4
     --apiserver-advertise-address=172.16.10.20 shall explictly select the IP address for apiserver !!!
     note: 172.16.10.20 is used by baremetal wyou
           172.16.0.8   is used by vm        vwyou
     note: permanently add ip address by nmcli as guideline:  git@github.com:wt2017/linux-command-note.git
                                                              -> nmcli-add-ipaddress-on-interface
     

     #2.2.5
     kubeadm reset is a good way to revert things setted by kubeadm init, including certs, IPs, ...

     Successful result example:
     "
         Your Kubernetes control-plane has initialized successfully!

         To start using your cluster, you need to run the following as a regular user:

           mkdir -p $HOME/.kube
           sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
           sudo chown $(id -u):$(id -g) $HOME/.kube/config

         Alternatively, if you are the root user, you can run:

           export KUBECONFIG=/etc/kubernetes/admin.conf

         You should now deploy a pod network to the cluster.
         Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
           https://kubernetes.io/docs/concepts/cluster-administration/addons/

         Then you can join any number of worker nodes by running the following on each as root:
    "

    #2.2.5
    By default, your cluster will not schedule Pods on the control plane nodes for security reasons. If you want to be able to schedule Pods on the control plane nodes, for example for a single machine Kubernetes cluster, run

    kubectl taint nodes --all node-role.kubernetes.io/control-plane-


#2.3 network : pod network creation : https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network
             kubernetes network model : https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model
             addon list : https://kubernetes.io/docs/concepts/cluster-administration/addons/
                          candidate solution : Multus + OVN-Kubernetes

     command: kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
     note: docker.io is blocked by PRC, then 
           2.3.1 curl -L -Ohttps://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml
           2.3.2 edit kube-flannel.yml, update docker.io to dockerproxy.cn, see updated kube-flannel.yml

#2.4 one work node joins cluster:

#2.4.1 disable firewalld:
         sudo systemctl stop firewalld; sudo systemctl disable firewalld;
#2.4.2 command:
         sudo kubeadm join 172.16.0.8:6443 --token csze99.gt6bk6xmryfd8h2u --discovery-token-unsafe-skip-ca-verification --node-name vfanny

         Note-1: take care same Pause Image Issue in #2.2.1
         Note-2: node-name is mandatory to distinguish nodes.
         Note-3: token has TTL, and will be aging out after TTL is reached.
                 kubeadm token list    ->  check token on master node
                 kubeadm token create  ->  create new token if the legacy one is aging out
         Note-4: --discovery-token-unsafe-skip-ca-verification can be used to skip ca verification but token sufficient.


#3 nvidia : 
#3.1 install nvidia toolkit
     #3.1.1 uninstall legacy version
         sudo dnf remove cuda
         sudo dnf autoremove
     #3.3.2 download correct version of nvidia toolkit : https://developer.nvidia.com/cuda-toolkit-archive
         install according to guide, e.g.https://developer.nvidia.com/cuda-12-4-0-download-archive?target_os=Linux&target_arch=x86_64&Distribution=Fedora&target_version=39&target_type=rpm_local

     #3.3.3 configure runtime
         nvidia-ctk runtime configure --runtime=crio
         or
         nvidia-ctk runtime configure --runtime=crio --set-as-default --config=/etc/crio/crio.conf.d/99-nvidia.conf
     #3.3.4 systemctl restart crio


#3.3 gpu-operator: https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/getting-started.html#operator-install-guide
     #3.3.1 node-feature-discovery:
            NOTE: no need for node-feature-discovery-operator!

            git clone https://github.com/kubernetes-sigs/node-feature-discovery
            kubectl apply -k deployment/overlays/default
     #3.3.2 gpu-operator:
            NOTE: opt-1 is valid; opt-2 is invalid yet
            opt-1: helm repo add nvidia https://helm.ngc.nvidia.com/nvidia && helm repo update
            opt-2: git clone https://github.com/NVIDIA/gpu-operator.git && cd /home/wyou/Kubernetes/nvidia/gpu-operator/deployments/gpu-operator

            helm install --wait --generate-name -n gpu-operator --create-namespace nvidia/gpu-operator --set driver.enabled=false --set toolkit.enabled=false --set cdi.enabled=true --set cdi.default=true --set nfd.enabled=false
     #3.3.3 time slicing: https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-sharing.html#applying-one-cluster-wide-configuration
            note: Applying One Cluster-Wide Configuration section is valid for wyou env
                  command below is mandatory to make multiple slice visible by "kubectl describe nodes":
                     #3.3.3.1
                     kubectl create -n gpu-operator -f time-slicing-config-all.yaml -> time-slicing-config-all.yaml is in folder /home/wyou/Kubernetes/nvidia/gpu-time-slice

                     #3.3.3.2
                     kubectl patch clusterpolicies.nvidia.com/cluster-policy \
                                                -n gpu-operator --type merge \
                                                -p '{"spec": {"devicePlugin": {"config": {"name": "time-slicing-config-all", "default": "any"}}}}'

#note:
     docker : to stop docker service : systemctl stop docker.socket (stop docker.service can't permanently stop it)
     containerd : to stop containerd service : systemctl stop containerd.service
     helm uninstall:
          a) helm list -A  -> list all helm versions
          b) helm uninstall <version (1st column of a) result)> -n namespace

#4 permission
     chmod 644 /etc/kubernetes/admin.conf
#5 untainted control-plane node
     kubectl taint nodes --all node-role.kubernetes.io/control-plane-


FINAL CLUSTER:(kubectl get pods --all-namespace)
NAMESPACE                NAME                                   READY   STATUS      RESTARTS   AGE
gpu-operator             gpu-feature-discovery-6v2tv            1/1     Running     0          5m21s
gpu-operator             gpu-operator-5df7f9b8c6-z74ts          1/1     Running     0          5m40s
gpu-operator             nvidia-cuda-validator-mfck9            0/1     Completed   0          5m7s
gpu-operator             nvidia-dcgm-exporter-5r6z8             1/1     Running     0          5m21s
gpu-operator             nvidia-device-plugin-daemonset-t8j9p   1/1     Running     0          5m21s
gpu-operator             nvidia-operator-validator-hs2z8        1/1     Running     0          5m21s
kube-flannel             kube-flannel-ds-xkf6f                  1/1     Running     30         45d
kube-system              coredns-5f98f8d567-flpwn               1/1     Running     30         45d
kube-system              coredns-5f98f8d567-j27gk               1/1     Running     30         45d
kube-system              etcd-fedora                            1/1     Running     443        45d
kube-system              kube-apiserver-fedora                  1/1     Running     417        45d
kube-system              kube-controller-manager-fedora         1/1     Running     40         45d
kube-system              kube-proxy-jlgws                       1/1     Running     30         45d
kube-system              kube-scheduler-fedora                  1/1     Running     42         45d
node-feature-discovery   nfd-gc-64b58969cb-jhrrk                1/1     Running     0          52m
node-feature-discovery   nfd-master-7ccccd5868-n4lwk            1/1     Running     0          52m
node-feature-discovery   nfd-worker-kvq6t                       1/1     Running     0          52m

