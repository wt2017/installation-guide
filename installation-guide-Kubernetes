#1 installation overview : https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/

#1.1 cri-o : source based compilation and installation :
               https://github.com/cri-o/cri-o/blob/main/install.md#build-and-install-cri-o-from-source
               https://github.com/cri-o/cri-o/blob/main/install.md#starting-cri-o
                     If you are installing for the first time, generate and install configuration files with: sudo make install.config
                     You can run it manually there, or you can set up a systemd unit file with: sudo make install.systemd
     Note:
         podman is the tool for cri-o !!! don't use docker !
         replace every docker command with podman !
         It's also important to execute podman as root since rootless images are stored somewhere else !

#1.2 go : install go in /usr/local/go : https://go.dev/doc/install

#1.3 cni : compile and install cni : https://github.com/cri-o/cri-o/blob/main/contrib/cni/README.md

#1.4 swap : turn off permenently : https://unix.stackexchange.com/questions/674196/cannot-disable-swap

[optional] #1.5 kubectl : install guide : https://kubernetes.io/docs/tasks/tools/
Rationale: step #1.6 includes kubectl together

#1.6 kubeadm kubelet kubectl : install guide : https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/install-kubeadm/
                                     api guide : https://kubernetes.io/docs/reference/setup-tools/kubeadm/
     command: sudo yum install -y kubelet kubeadm kubectl --disableexcludes=kubernetes

[nothing to do] #1.7 cgroup driver : update cgroupDriver value from cgroupfs to systemd
Rationale: In v1.22 and later, if the user does not set the cgroupDriver field under KubeletConfiguration, kubeadm defaults it to systemd




#2. cluster creation overview : https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/
#2.1 kernel module installation:
            cat <<EOF | sudo tee /etc/modules-load.d/k8s.conf
            > overlay
            > bridge
            > br_netfilter
            > EOF

            cat <<EOF | sudo tee /etc/sysctl.d/k8s.conf
            > net.bridge.bridge-nf-call-iptables  = 1
            > net.bridge.bridge-nf-call-ip6tables = 1
            > net.ipv4.ip_forward                 = 1
            > EOF


#2.2 kubeadm init --image-repository=registry.cn-hangzhou.aliyuncs.com/google_containers --pod-network-cidr=10.244.0.0/16 --apiserver-advertise-address=172.168.0.8
     #2.2.1
     PauseImage repository can't be changed by parameter --image-repository, so one work around is made as.
     Note: have a try w/o below workaround since it could be fixed by later released code.

           wyou@fedora:~/Kubernetes/crio/cri-o$ git diff
           diff --git a/internal/storage/runtime.go b/internal/storage/runtime.go
           index 27f9dac74..c47b7c1c4 100644
           --- a/internal/storage/runtime.go
           +++ b/internal/storage/runtime.go
           @@ -320,6 +320,7 @@ func (r *runtimeService) CreatePodSandbox(systemContext *types.SystemContext, po
                                   DestinationCtx: systemContext,
                           })
                           if err != nil {
           +                       logrus.Errorf("WYOU: Failed to PullImage %q", pauseImage)
                                   return ContainerInfo{}, err
                           }
                           _, img, err = r.storageTransport.ResolveReference(ref)
           diff --git a/pkg/config/config.go b/pkg/config/config.go
           index a5b740611..72119b8c7 100644
           --- a/pkg/config/config.go
           +++ b/pkg/config/config.go
           @@ -1424,6 +1424,8 @@ func (c *ImageConfig) Validate(onExecution bool) error {
 
            // ParsePauseImage parses the .PauseImage value as into a validated, well-typed value.
            func (c *ImageConfig) ParsePauseImage() (references.RegistryImageReference, error) {
           +       // WYOU : hardcode the PauseImage repository as workaround
           +       c.PauseImage = "registry.cn-hangzhou.aliyuncs.com/google_containers/pause:3.9"
                   return references.ParseRegistryImageReferenceFromOutOfProcessData(c.PauseImage)
            }

     #2.2.2
     --pod-network-cidr=10.244.0.0/16 is mandatory for flannel network cni !!!

     #2.2.3
     --apiserver-advertise-address=172.16.0.8 shall explictly select the IP address for apiserver !!!

     #2.2.4
     kubeadm reset is a good way to revert things setted by kubeadm init, including certs, IPs, ...

     Successful result example:
     "
         Your Kubernetes control-plane has initialized successfully!

         To start using your cluster, you need to run the following as a regular user:

           mkdir -p $HOME/.kube
           sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
           sudo chown $(id -u):$(id -g) $HOME/.kube/config

         Alternatively, if you are the root user, you can run:

           export KUBECONFIG=/etc/kubernetes/admin.conf

         You should now deploy a pod network to the cluster.
         Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
           https://kubernetes.io/docs/concepts/cluster-administration/addons/

         Then you can join any number of worker nodes by running the following on each as root:
    "


#2.3 network : pod network creation : https://kubernetes.io/docs/setup/production-environment/tools/kubeadm/create-cluster-kubeadm/#pod-network
             kubernetes network model : https://kubernetes.io/docs/concepts/cluster-administration/networking/#how-to-implement-the-kubernetes-networking-model
             addon list : https://kubernetes.io/docs/concepts/cluster-administration/addons/
                          candidate solution : Multus + OVN-Kubernetes

     command: kubectl apply -f https://github.com/flannel-io/flannel/releases/latest/download/kube-flannel.yml

#2.4 one work node joins cluster:

#2.4.1 disable firewalld:
         sudo systemctl stop firewalld; sudo systemctl disable firewalld;
#2.4.2 command:
         sudo kubeadm join 172.16.0.8:6443 --token csze99.gt6bk6xmryfd8h2u --discovery-token-unsafe-skip-ca-verification --node-name vfanny

         Note-1: take care same Pause Image Issue in #2.2.1
         Note-2: node-name is mandatory to distinguish nodes.
         Note-3: token has TTL, and will be aging out after TTL is reached.
                 kubeadm token list    ->  check token on master node
                 kubeadm token create  ->  create new token if the legacy one is aging out
         Note-4: --discovery-token-unsafe-skip-ca-verification can be used to skip ca verification but token sufficient.


#3 nvidia crio : 
#3.1 curl -s -L https://nvidia.github.io/libnvidia-container/stable/rpm/nvidia-container-toolkit.repo | \
     sudo tee /etc/yum.repos.d/nvidia-container-toolkit.repo

#3.2 dnf install -y nvidia-container-toolkit
     nvidia-ctk runtime configure --runtime=crio
     or
     nvidia-ctk runtime configure --runtime=crio --set-as-default --config=/etc/crio/crio.conf.d/99-nvidia.conf
     systemctl restart crio
#3.3 gpu-operator: https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/getting-started.html#operator-install-guide
     #3.3.1 node-feature-discovery: (no need for node-feature-discovery-operator)
            git clone https://github.com/kubernetes-sigs/node-feature-discovery
            kubectl apply -k deployment/overlays/default
     #3.3.2 time slicing: https://docs.nvidia.com/datacenter/cloud-native/gpu-operator/latest/gpu-sharing.html 
     #3.3.3 gpu-operator:
            cd /home/wyou/Kubernetes/nvidia/gpu-operator/deployments/gpu-operator
            helm install --wait --generate-name -n gpu-operator --create-namespace nvidia/gpu-operator --set driver.enabled=false --set toolkit.enabled=false --set cdi.enabled=true --set nfd.enabled=false --set devicePlugin.config.name=time-slicing-config-all

            note: --set devicePlugin.config.name=time-slicing-config-all  -> configmap for gpu-time-slice

#note:
     docker : to stop docker service : systemctl stop docker.socket (stop docker.service can't permanently stop it)
     containerd : to stop containerd service : systemctl stop containerd.service
     helm uninstall:
          a) helm list -A  -> list all helm versions
          b) helm uninstall <version (1st column of a) result)> -n namespace

#4 permission
     chmod 644 /etc/kubernetes/admin.conf
#5 untainted control-plane node
     kubectl taint nodes --all node-role.kubernetes.io/control-plane-


FINAL CLUSTER:(kubectl get pods --all-namespace)
NAMESPACE                NAME                                   READY   STATUS      RESTARTS   AGE
gpu-operator             gpu-feature-discovery-6v2tv            1/1     Running     0          5m21s
gpu-operator             gpu-operator-5df7f9b8c6-z74ts          1/1     Running     0          5m40s
gpu-operator             nvidia-cuda-validator-mfck9            0/1     Completed   0          5m7s
gpu-operator             nvidia-dcgm-exporter-5r6z8             1/1     Running     0          5m21s
gpu-operator             nvidia-device-plugin-daemonset-t8j9p   1/1     Running     0          5m21s
gpu-operator             nvidia-operator-validator-hs2z8        1/1     Running     0          5m21s
kube-flannel             kube-flannel-ds-xkf6f                  1/1     Running     30         45d
kube-system              coredns-5f98f8d567-flpwn               1/1     Running     30         45d
kube-system              coredns-5f98f8d567-j27gk               1/1     Running     30         45d
kube-system              etcd-fedora                            1/1     Running     443        45d
kube-system              kube-apiserver-fedora                  1/1     Running     417        45d
kube-system              kube-controller-manager-fedora         1/1     Running     40         45d
kube-system              kube-proxy-jlgws                       1/1     Running     30         45d
kube-system              kube-scheduler-fedora                  1/1     Running     42         45d
node-feature-discovery   nfd-gc-64b58969cb-jhrrk                1/1     Running     0          52m
node-feature-discovery   nfd-master-7ccccd5868-n4lwk            1/1     Running     0          52m
node-feature-discovery   nfd-worker-kvq6t                       1/1     Running     0          52m

